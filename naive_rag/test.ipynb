{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170c5841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c076de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7fd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Loading documents from directory ====\n",
      "Loaded 21 documents from ./news_articles\n"
     ]
    }
   ],
   "source": [
    "# Function to load documents from a directory\n",
    "def load_documents_from_directory(directory_path):\n",
    "    print(\"==== Loading documents from directory ====\")\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(\n",
    "                os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\"\n",
    "            ) as file:\n",
    "                documents.append({\"id\": filename, \"text\": file.read()})\n",
    "    return documents\n",
    "\n",
    "# Function to load documents from a file\n",
    "directory_path = './news_articles'\n",
    "documents = load_documents_from_directory(directory_path)\n",
    "print(f\"Loaded {len(documents)} documents from {directory_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49ca33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n",
      "==== Splitting docs into chunks ====\n"
     ]
    }
   ],
   "source": [
    "#Function to Split Documents into chunks\n",
    "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "#splitting the documents into chunks\n",
    "chunked_documents = []\n",
    "for doc in documents:\n",
    "    chunks = split_text(doc[\"text\"])\n",
    "    print(\"==== Splitting docs into chunks ====\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a70dce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2f2d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from chromadb.utils.embedding_functions import EmbeddingFunction\n",
    "\n",
    "# Wrapper to make LangChain HuggingFaceEmbeddings compatible with ChromaDB\n",
    "class ChromaCompatibleHFEmbedding(EmbeddingFunction):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self.model.embed_documents(input)\n",
    "\n",
    "# Instantiate the wrapper\n",
    "embedding_function = ChromaCompatibleHFEmbedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f83c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize the Chroma client with persistence\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma_persistent_storage\")\n",
    "collection_name = \"document_qa_collection\"\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name, embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e199cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "def generate_and_store_embeddings(documents, embedding_function, collection):\n",
    "    print(\"==== Generating embeddings and storing in ChromaDB ====\")\n",
    "\n",
    "    ids = []\n",
    "    texts = []\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in documents:\n",
    "        ids.append(doc[\"id\"])\n",
    "        texts.append(doc[\"text\"])\n",
    "        embedding = embedding_function([doc[\"text\"]])[0]  # Proper call\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Stored {len(texts)} chunks in ChromaDB.\")\n",
    "\n",
    "\n",
    "\n",
    "# Assume embeddings and collection are already initialized\n",
    "#generate_and_store_embeddings(chunked_documents, embedding_function, collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c07e805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Generating embeddings and storing in ChromaDB ====\n",
      "✅ Stored 184 chunks in ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assume embeddings and collection are already initialized\n",
    "generate_and_store_embeddings(chunked_documents, embedding_function, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f02ee192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to query documents\n",
    "def query_documents(question, n_results=2):\n",
    "    # query_embedding = get_openai_embedding(question)\n",
    "    results = collection.query(query_texts=question, n_results=n_results)\n",
    "\n",
    "    # Extract the relevant chunks\n",
    "    relevant_chunks = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "    print(\"==== Returning relevant chunks ====\")\n",
    "    return relevant_chunks\n",
    "    # for idx, document in enumerate(results[\"documents\"][0]):\n",
    "    #     doc_id = results[\"ids\"][0][idx]\n",
    "    #     distance = results[\"distances\"][0][idx]\n",
    "    #     print(f\"Found document chunk: {document} (ID: {doc_id}, Distance: {distance})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfcc52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the llm\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                            google_api_key=GOOGLE_API_KEY , \n",
    "                            temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a00b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
    "\n",
    "def generate_response(question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following pieces of \"\n",
    "        \"retrieved context to answer the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\nContext:\\n\" + context + \"\\n\\nQuestion:\\n\" + question\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt)  # LangChain's way\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75b99208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Returning relevant chunks ====\n",
      "Databricks acquired Okera, a data governance platform focused on AI, to improve its Unity Catalog governance solution.  This acquisition addresses the challenges of managing rapidly expanding data assets in the age of LLMs.  Databricks also plans to integrate Okera's technology and expose additional APIs for its data governance partners.\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "# query_documents(\"tell me about AI replacing TV writers strike.\")\n",
    "# Example query and response generation\n",
    "question = \"tell me about databricks\"\n",
    "relevant_chunks = query_documents(question)\n",
    "answer = generate_response(question, relevant_chunks)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c254c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
